{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dc34944",
   "metadata": {},
   "source": [
    "# Python Course - Tutorial 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399dcac0",
   "metadata": {},
   "source": [
    "### Exercise 1 (Sharpe Ratio Comparison)\n",
    "In the field of finance, the **Sharpe Ratio** is a common metric used to understand how well an investment performs relative to its risk. Formally, the Sharpe Ratio is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Sharpe Ratio} = \\frac{R_p - R_f}{\\sigma_p}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $R_p$ is the expected return (often taken as the mean return of the asset over a given period).  \n",
    "- $R_f$ is the risk-free rate (a theoretical return of an investment with zero risk, often approximated by government bonds).  \n",
    "- $\\sigma_p$ is the standard deviation (volatility) of the asset’s returns.\n",
    "\n",
    "A higher Sharpe Ratio indicates that the investment is providing a higher return per unit of risk.\n",
    "\n",
    "---\n",
    "\n",
    "**The datasets required for this exercise are provided in the `data/` directory:**\n",
    "\n",
    "- `data/SP500_Total_Return.csv`: Historical data for the S&P 500 Total Return Index  \n",
    "- `data/DAX.csv`: Historical data for the DAX Index  \n",
    "\n",
    "**Your tasks are to:**  \n",
    "\n",
    "(i) Load both datasets from the CSV files into Pandas DataFrames.  \n",
    "\n",
    "(ii) Compute the daily returns for each index (use e.g., Pandas `pct_change()` method).  \n",
    "\n",
    "(iii) Assume a **constant annualized risk-free rate of 2%** for this analysis. (In practice, you might load this from a dataset or a more appropriate proxy.)  \n",
    "\n",
    "(iv) Compute the annualized Sharpe Ratio for each index using:\n",
    "\n",
    "$$\n",
    "\\text{Sharpe Ratio (annualized)} = \\frac{(\\text{mean daily return} - \\text{daily risk-free rate}) \\times 252}{\\text{daily return volatility} \\times \\sqrt{252}}\n",
    "$$\n",
    "\n",
    "Here, $252$ represents the approximate number of trading days in a year.\n",
    "\n",
    "(v) Print out the Sharpe Ratio for both the S&P 500 Total Return and the DAX, and compare which one has a higher risk-adjusted return.\n",
    "\n",
    "**Note:** If a particular index has no data or returns are NaN, handle it gracefully and report that the Sharpe Ratio cannot be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa7218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b84624d",
   "metadata": {},
   "source": [
    "### Exercise 2: Tidy Data\n",
    "\n",
    "#### Overview: Wickham's Tidy Data Framework\n",
    "Hadley Wickham's seminal paper on [Tidy Data](https://www.jstatsoft.org/article/view/v059i10) focuses on structuring datasets to facilitate analysis. The principles are:\n",
    "1. **Each variable forms a column.**\n",
    "2. **Each observation forms a row.**\n",
    "3. **Each type of observational unit forms a table.**\n",
    "\n",
    "Tidy datasets ensure a consistent structure, simplifying manipulation, visualization, and modeling. However, real-world datasets often deviate from this structure, requiring transformation.\n",
    "\n",
    "#### Common Problems in Untidy Data\n",
    "Wickham identifies five types of untidy data:\n",
    "1. **Column headers are values, not variable names.**\n",
    "2. **Multiple variables are stored in one column.**\n",
    "3. **Variables are stored in both rows and columns.**\n",
    "4. **Multiple types of observational units are stored in the same table.**\n",
    "5. **One type of observational unit is spread out over multiple tables or files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f82835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f39ad9",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Column Headers Are Values, Not Variable Names - Part 1\n",
    "\n",
    "In the Pew dataset column headers represent income brackets rather than variable names. Our goal is to restructure the data into a tidy format, with columns for `religion`, `income`, and `frequency`.\n",
    "\n",
    "#### Tasks:\n",
    "1. Load the Pew dataset from a CSV file.\n",
    "2. Inspect the data to identify untidy elements.\n",
    "3. Use Pandas' `melt` function to:\n",
    "   - Transform the income brackets (column headers) into a new column named `income`.\n",
    "   - Ensure the original `religion` column remains intact.\n",
    "   - Create a new column named `frequency` to hold the corresponding values.\n",
    "4. Sort the resulting tidy dataset by `religion` (alphabetically) and `frequency` (numerically).\n",
    "\n",
    "\n",
    "#### Dataset Preview:\n",
    "\n",
    "The untidy data might look like this:\n",
    "\n",
    "| religion           | \\<$10k | \\$10–20k | \\$20–30k | \\$30–40k | \\$40–50k | \\$50–75k |\n",
    "|--------------------|--------|---------|---------|---------|---------|---------|\n",
    "| Agnostic           | 27     | 34      | 60      | 81      | 76      | 137     |\n",
    "| Atheist            | 12     | 27      | 37      | 52      | 35      | 70      |\n",
    "| Buddhist           | 27     | 21      | 30      | 34      | 33      | 58      |\n",
    "| Catholic           | 418    | 617     | 732     | 670     | 638     | 1116    |\n",
    "| Don’t know/refused | 15     | 14      | 15      | 11      | 10      | 35      |\n",
    "\n",
    "Using `pd.melt`, we can achieve the following tidy dataset:\n",
    "\n",
    "| religion           | income   | frequency |\n",
    "|--------------------|----------|-----------|\n",
    "| Agnostic           | \\<\\$10k  | 27        |\n",
    "| Agnostic           | \\$10–20k | 34        |\n",
    "| Agnostic           | \\$20–30k | 60        |\n",
    "| Atheist            | \\<\\$10k  | 12        |\n",
    "| Atheist            | \\$10–20k | 27        |\n",
    "| Buddhist           | \\<\\$10k  | 27        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f384bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pew = pd.read_csv(\"data/pew_sample.csv\")\n",
    "\n",
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f58dc24",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Column Headers Are Values, Not Variable Names - Part 2\n",
    "\n",
    "The **Billboard dataset** is stored in a wide format. The data contains weekly rankings (`wk1`, `wk2`, ..., `wk75`) spread across columns. Your task is to transform it into a **long format** where each row corresponds to a specific song's ranking in a particular week.\n",
    "\n",
    "#### Tasks:\n",
    "1. **Inspect the Dataset**: Familiarize yourself with the structure of the Billboard dataset. Identify columns to retain (e.g., `year`, `artist`, `track`, `time`, `date.entered`) and those to melt (e.g., `wk1`, `wk2`, ..., `wk75`).\n",
    "2. **Use `pd.melt`**:\n",
    "   - Reshape the dataset so that all weekly ranking columns (`wk1`, `wk2`, ..., `wk75`) are combined into a single `week` column.\n",
    "   - Create a new column `rank` to hold the corresponding ranking values.\n",
    "3. **Clean Up the Week Column**:\n",
    "   - Use string methods to extract only the numeric part of the week values (e.g., `wk1` → `1`).\n",
    "   - Convert the `week` column into an integer type for easier manipulation.\n",
    "\n",
    "**Hint:** Apply the `.str.replace()` or `.str.extract()` methods to clean up the `week` column.\n",
    "\n",
    "#### Dataset Preview:\n",
    "**Original Untidy Data (Wide Format)**:\n",
    "\n",
    "| year | artist       | track                    | time | date.entered | wk1 | wk2 | wk3 | ... | wk75 |\n",
    "|------|--------------|--------------------------|------|--------------|-----|-----|-----|-----|------|\n",
    "| 2000 | 2 Pac        | Baby Don’t Cry           | 4:22 | 2000-02-26   | 87  | 82  | 72  | ... | NaN  |\n",
    "| 2000 | 2Ge+her      | The Hardest Part Of ...  | 3:15 | 2000-09-02   | 91  | 87  | 82  | ... | NaN  |\n",
    "| 2000 | 3 Doors Down | Kryptonite               | 3:53 | 2000-04-08   | 81  | 70  | 66  | ... | NaN  |\n",
    "\n",
    "\n",
    "**After Transformation (Long Format)**:\n",
    "\n",
    "| year | artist       | time | track                  | date       | week | rank |\n",
    "|------|--------------|------|------------------------|------------|------|------|\n",
    "| 2000 | 2 Pac        | 4:22 | Baby Don’t Cry         | 2000-02-26 | 1    | 87   |\n",
    "| 2000 | 2 Pac        | 4:22 | Baby Don’t Cry         | 2000-03-04 | 2    | 82   |\n",
    "| 2000 | 2 Pac        | 4:22 | Baby Don’t Cry         | 2000-03-11 | 3    | 72   |\n",
    "| 2000 | 2Ge+her      | 3:15 | The Hardest Part Of ...| 2000-09-02 | 1    | 91   |\n",
    "| 2000 | 2Ge+her      | 3:15 | The Hardest Part Of ...| 2000-09-09 | 2    | 87   |\n",
    "| 2000 | 2Ge+her      | 3:15 | The Hardest Part Of ...| 2000-09-02 | 3    | 92   |\n",
    "| 2000 | 3 Doors Down | 3:53 | Kryptonite             | 2000-04-08 | 1    | 81   |\n",
    "| 2000 | 3 Doors Down | 3:53 | Kryptonite             | 2000-04-15 | 2    | 70   |\n",
    "| 2000 | 3 Doors Down | 3:53 | Kryptonite             | 2000-04-22 | 3    | 66   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "billboard = pd.read_csv(\"data/billboard.csv\")\n",
    "\n",
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7cf5c3",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Multiple Variables Are Stored in One Column\n",
    "\n",
    "In the tuberculosis (TB) dataset multiple variables (e.g., sex and age group) are combined into a single column. Your goal is to separate these into distinct columns, making the dataset tidy.\n",
    "\n",
    "\n",
    "#### Tasks:\n",
    "1. **Reshape the Dataset**: Use `pd.melt` to convert the wide format into a long format where the demographic variables (e.g., `m014`, `f1524`) are stored in a single column.\n",
    "2. **Split the Combined Column**: Extract the `sex` and `age` variables from the demographic column.\n",
    "3. **Clean the Age Values**: Use the `map` method with a dictionary to convert the age codes (e.g., `014`, `1524`) into human-readable ranges (e.g., `0–14`, `15–24`).\n",
    "\n",
    "\n",
    "#### Dataset Preview:\n",
    "**Original Untidy Data**:\n",
    "\n",
    "| country | year | m014 | m1524 | m2534 | m3544 | m4554 | m5564 | m65 | mu  | f014 | f1524 | f2534 |\n",
    "|---------|------|------|-------|-------|-------|-------|-------|-----|-----|------|-------|-------|\n",
    "| AD      | 2000 | 0    | 0     | 1     | 0     | 0     | 0     | 0   | --- | ---  | ---   | ---   |\n",
    "| AE      | 2000 | 2    | 4     | 4     | 6     | 5     | 12    | 10  | --- | 3    | 6     | 5     |\n",
    "| AF      | 2000 | 52   | 228   | 183   | 149   | 129   | 94    | 80  | --- | 93   | 142   | 128   |\n",
    "\n",
    "**After Transformation (Tidy Format)**:\n",
    "\n",
    "| country | year | sex | age  | cases |\n",
    "|---------|------|-----|------|-------|\n",
    "| AD      | 2000 | m   | 0–14 | 0     |\n",
    "| AE      | 2000 | m   | 0–14 | 2     |\n",
    "| AF      | 2000 | m   | 0–14 | 52    |\n",
    "| AD      | 2000 | f   | 15–24| 0     |\n",
    "| AE      | 2000 | f   | 15–24| 6     |\n",
    "| AF      | 2000 | f   | 15–24| 142   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65584db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = pd.read_csv(\"data/tb_sample.csv\")\n",
    "\n",
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c8d419",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Variables Are Stored in Both Rows and Columns (Bonus!)\n",
    "\n",
    "The weather dataset contains variables such as minimum and maximum temperature (`tmin`, `tmax`) that stored across both rows and columns. Your task is to tidy the dataset by ensuring each variable is represented in its own column.\n",
    "\n",
    "\n",
    "\n",
    "#### Tasks:\n",
    "1. **Reshape the Dataset**:\n",
    "   - Use `pd.melt` to gather all day columns (`d1`, `d2`, ..., `d31`) into a single column, converting the wide format into a long format.\n",
    "   - Create a new `date` column by combining `year`, `month`, and the day extracted from the melted column.\n",
    "\n",
    "2. **Separate Variables**:\n",
    "   - Pivot the data so that `tmin` and `tmax` are stored as separate columns, with each row representing a unique `date`.\n",
    "\n",
    "3. **Handle Missing Values**:\n",
    "   - Remove rows where the `value` is missing (`—`).\n",
    "\n",
    "**Hints:**\n",
    "- Combine `year`, `month`, and `day` into a single `date` column using `pd.to_datetime` or string formatting.\n",
    "- Use `pivot` or `unstack` to move the `element` values (`tmin`, `tmax`) into separate columns.\n",
    "\n",
    "#### Dataset Preview\n",
    "**Original Untidy Data**:\n",
    "\n",
    "| id      | year | month | element | d1   | d2   | d3   | ... | d8  |\n",
    "|---------|------|-------|---------|------|------|------|-----|------|\n",
    "| MX17004 | 2010 | 1     | tmax    | —    | —    | —    | ... | —    |\n",
    "| MX17004 | 2010 | 1     | tmin    | —    | —    | —    | ... | —    |\n",
    "| MX17004 | 2010 | 2     | tmax    | —    | 27.3 | 24.1 | ... | —    |\n",
    "| MX17004 | 2010 | 2     | tmin    | —    | 14.4 | 14.4 | ... | —    |\n",
    "\n",
    "**After Transformation (Tidy Format)**:\n",
    "\n",
    "| id      | date       | tmax  | tmin  |\n",
    "|---------|------------|-------|-------|\n",
    "| MX17004 | 2010-02-02 | 27.3  | 14.4  |\n",
    "| MX17004 | 2010-02-03 | 24.1  | 14.4  |\n",
    "| MX17004 | 2010-03-05 | 32.1  | 14.2  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ec649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6667c59",
   "metadata": {},
   "source": [
    "### Exercise 2.4: Multiple Types of Observational Units Are Stored in the Same Table\n",
    "\n",
    "The Billboard dataset, contains information about tracks (e.g., artist, track title, and time) as well as their rankings over time. These represent two distinct types of observational units. Your goal is to split the dataset into two separate tables:\n",
    "1. One table for track information.\n",
    "2. Another table for weekly rankings.\n",
    "\n",
    "\n",
    "#### Steps to Complete:\n",
    "\n",
    "1. **Extract Track Information**: Identify the unique combinations of `artist`, `track`, and `time` and assign each unique combination a unique `id`.\n",
    "2. **Separate Weekly Rankings**: Create a new table for rankings, using the `id` from the track table to link the two datasets.\n",
    "3. **Join Tables When Needed**: Use the `merge` function to link track information with ranking data when required for analysis.\n",
    "\n",
    "**Hint:** Use `pd.drop_duplicates` to extract unique rows for the track table.\n",
    "\n",
    "\n",
    "#### Dataset Preview\n",
    "**Original Untidy Data** (Mixed Observational Units):\n",
    "\n",
    "| year | artist          | track              | time | date       | week | rank |\n",
    "|------|-----------------|--------------------|------|------------|------|------|\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-04-29 | 1    | 100  |\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-05-06 | 2    | 99   |\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-05-13 | 3    | 96   |\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-05-20 | 4    | 76   |\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-05-27 | 5    | 55   |\n",
    "\n",
    "\n",
    "**After Transformation: Two Separate Tables**\n",
    "\n",
    "1. **Track Table** (One row per unique track):\n",
    "\n",
    "| id | artist          | track              | time |\n",
    "|----|-----------------|--------------------|------|\n",
    "| 1  | Nelly           | Country Grammar    | 4:17 |\n",
    "\n",
    "2. **Rank Table** (One row per weekly ranking):\n",
    "\n",
    "| id | date       | rank |\n",
    "|----|------------|------|\n",
    "| 1  | 2000-04-29 | 100  |\n",
    "| 1  | 2000-05-06 | 99   |\n",
    "| 1  | 2000-05-13 | 96   |\n",
    "| 1  | 2000-05-20 | 76   |\n",
    "| 1  | 2000-05-27 | 55   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the billboard dataframe from exercise 1.2, after transforming it into a long format.\n",
    "\n",
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53274782",
   "metadata": {},
   "source": [
    "### Exercise 3: Visualizing the Palmer Penguins Dataset with Seaborn\n",
    "\n",
    "The Palmer Penguins dataset provides information about three penguin species—Adélie, Chinstrap, and Gentoo—observed in Antarctica. This dataset contains details such as body mass, flipper length, and bill dimensions, making it ideal for data visualization practice.\n",
    "\n",
    "#### Dataset Overview\n",
    "The dataset includes the following key columns:\n",
    "- `species`: The penguin species (Adélie, Chinstrap, Gentoo).\n",
    "- `island`: The island where the penguins were observed.\n",
    "- `bill_length_mm`: Length of the penguin's bill (in millimeters).\n",
    "- `bill_depth_mm`: Depth of the penguin's bill (in millimeters).\n",
    "- `flipper_length_mm`: Length of the penguin's flipper (in millimeters).\n",
    "- `body_mass_g`: Body mass of the penguin (in grams).\n",
    "- `sex`: The penguin's sex (male, female).\n",
    "\n",
    "The dataset can be loaded directly from the seaborn repository using the URL with `pd.read_csv(url)`:  \n",
    "<https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv>\n",
    "\n",
    "In the previous tutorial, we briefly explored how to create plots using **matplotlib**. Today, we will focus on **seaborn**, a powerful Python library built on top of matplotlib that allows us to create elegant and informative visualizations with minimal effort. By using seaborn, we can generate complex and aesthetically pleasing plots with just a few lines of code.\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. **Visualize the Distribution of Flipper Length**:\n",
    "   - Create a histogram or kernel density plot (KDE) of `flipper_length_mm`, with the distributions differentiated by `species` using the `hue` parameter.\n",
    "\n",
    "2. **Explore the Relationship Between Body Mass and Flipper Length**:\n",
    "   - Use a scatter plot to visualize the relationship between `body_mass_g` and `flipper_length_mm`.\n",
    "   - Differentiate the points by `species` using the `hue` parameter and use distinct markers for each species.\n",
    "\n",
    "3. **Compare Body Mass Across Species**:\n",
    "   - Create a boxplot to compare the distributions of `body_mass_g` across the three penguin species.\n",
    "\n",
    "4. **Analyze Pairwise Relationships**:\n",
    "   - Generate a pair plot for the numerical variables (`bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`), with the points colored by `species`. Use a KDE plot for the diagonal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c446412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
